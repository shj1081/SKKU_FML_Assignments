{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A9 : Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Calculate the outputs of the given data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all activation functions are [Hardlim]\n",
      "\n",
      "x1 = 0, x2 = 0, y = 0\n",
      "x1 = 0, x2 = 1, y = 1\n",
      "x1 = 1, x2 = 0, y = 0\n",
      "x1 = 1, x2 = 1, y = 0\n",
      "\n",
      "all activation functions are [sigmoid]\n",
      "\n",
      "x1 = 0, x2 = 0, y = 0.42436377623451826\n",
      "x1 = 0, x2 = 1, y = 0.4905304217782711\n",
      "x1 = 1, x2 = 0, y = 0.3775406687981454\n",
      "x1 = 1, x2 = 1, y = 0.42436377623451826\n",
      "\n",
      "all activation functions are [ReLU]\n",
      "\n",
      "x1 = 0, x2 = 0, y = 0\n",
      "x1 = 0, x2 = 1, y = 0.5\n",
      "x1 = 1, x2 = 0, y = 0\n",
      "x1 = 1, x2 = 1, y = 0.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define weights\n",
    "w = [1.5, 0.5, 0.5, 1.5, -1.5, -0.5, -1, 1, -0.5]\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def Hardlim(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def ReLU(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Calculate the output with the given activation function\n",
    "def calculate_output(x1, x2, activation_function):\n",
    "    s1 = w[0] * x1 + w[2] * x2 + w[4]\n",
    "    s2 = w[1] * x1 + w[3] * x2 + w[5]\n",
    "    s3 = w[6] * activation_function(s1) + w[7] * activation_function(s2) + w[8]\n",
    "    return activation_function(s3)\n",
    "\n",
    "# Function to display results\n",
    "def display_results(activation_function, function_name):\n",
    "    print(f\"\\nall activation functions are [{function_name}]\\n\")\n",
    "    for x1 in range(2):\n",
    "        for x2 in range(2):\n",
    "            y = calculate_output(x1, x2, activation_function)\n",
    "            print(f\"x1 = {x1}, x2 = {x2}, y = {y}\")\n",
    "\n",
    "# Display results for each activation function\n",
    "display_results(Hardlim, \"Hardlim\")\n",
    "display_results(sigmoid, \"sigmoid\")\n",
    "display_results(ReLU, \"ReLU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Update the weights of the given neural network using Gradient Descent method\n",
    "\n",
    "- We have one training sampl (1,1). \n",
    "- The initial weights are w1=1, w2 = 1. \n",
    "- The learning rate is 0.1. \n",
    "- The activation function is the sigmoid function.\n",
    "- The Loss function is the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First update: W1 = 1.0014015358528543, W2 = 1.0052113052936016\n",
      "Second update: W1 = 1.0028035692263884, W2 = 1.0104027514671299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "# Derivative of the MSE loss function\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Single function for forward pass, backward pass, and calculating gradients\n",
    "def calculate_grad(x, y_true, W1, W2, learning_rate):\n",
    "    # Forward pass\n",
    "    s1 = W1 * x\n",
    "    a1 = sigmoid(s1)\n",
    "    s2 = W2 * a1\n",
    "    y_pred = sigmoid(s2)\n",
    "    \n",
    "    # Backward pass\n",
    "    d_loss_y_pred = mse_loss_derivative(y_true, y_pred)\n",
    "    d_y_pred_s2 = sigmoid_derivative(y_pred)\n",
    "    d_s2_W2 = a1\n",
    "    d_s2_a1 = W2\n",
    "    d_a1_s1 = sigmoid_derivative(a1)\n",
    "    d_s1_W1 = x\n",
    "\n",
    "    grad_W2 = d_loss_y_pred * d_y_pred_s2 * d_s2_W2\n",
    "    grad_W1 = d_loss_y_pred * d_y_pred_s2 * d_s2_a1 * d_a1_s1 * d_s1_W1\n",
    "\n",
    "    return grad_W1, grad_W2\n",
    "\n",
    "def train_step(x, y_true, W1, W2, learning_rate):\n",
    "    grad_W1, grad_W2 = calculate_grad(x, y_true, W1, W2, learning_rate)\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    return W1, W2\n",
    "\n",
    "# Initial weights and learning rate\n",
    "W1 = 1\n",
    "W2 = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training sample\n",
    "x = 1\n",
    "y_true = 1\n",
    "\n",
    "# First update\n",
    "W1, W2 = train_step(x, y_true, W1, W2, learning_rate)\n",
    "first_update_W1 = W1\n",
    "first_update_W2 = W2\n",
    "\n",
    "# Second update\n",
    "W1, W2 = train_step(x, y_true, W1, W2, learning_rate)\n",
    "second_update_W1 = W1\n",
    "second_update_W2 = W2\n",
    "\n",
    "print(f\"First update: W1 = {first_update_W1}, W2 = {first_update_W2}\")\n",
    "print(f\"Second update: W1 = {second_update_W1}, W2 = {second_update_W2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Update the weights of the given neural network using Gradient Descent method\n",
    "\n",
    "- We have two training samples (1,1) and (0,0).\n",
    "- The initial weights are w1=1, w2 = 1. \n",
    "- The learning rate is 0.1. \n",
    "- The activation function is the sigmoid function.\n",
    "- The Loss function is the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First update: W1 = 1.0014015358528543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "# Derivative of the MSE loss function\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Single function for forward pass, backward pass, and calculating gradients\n",
    "def calculate_grad(x, y_true, W1, W2, learning_rate):\n",
    "    # Forward pass\n",
    "    s1 = W1 * x\n",
    "    a1 = sigmoid(s1)\n",
    "    s2 = W2 * a1\n",
    "    y_pred = sigmoid(s2)\n",
    "    \n",
    "    # Backward pass\n",
    "    d_loss_y_pred = mse_loss_derivative(y_true, y_pred)\n",
    "    d_y_pred_s2 = sigmoid_derivative(y_pred)\n",
    "    d_s2_W2 = a1\n",
    "    d_s2_a1 = W2\n",
    "    d_a1_s1 = sigmoid_derivative(a1)\n",
    "    d_s1_W1 = x\n",
    "\n",
    "    grad_W2 = d_loss_y_pred * d_y_pred_s2 * d_s2_W2\n",
    "    grad_W1 = d_loss_y_pred * d_y_pred_s2 * d_s2_a1 * d_a1_s1 * d_s1_W1\n",
    "\n",
    "    return grad_W1, grad_W2\n",
    "\n",
    "def train_step(x, y_true, W1, W2, learning_rate):\n",
    "    total_grad_W1 = 0\n",
    "    total_grad_W2 = 0\n",
    "    \n",
    "    # calculate the gradients for each training sample\n",
    "    for i in range(len(x)):\n",
    "        grad_W1, grad_W2 = calculate_grad(x[i], y_true[i], W1, W2, learning_rate)\n",
    "        total_grad_W1 += grad_W1\n",
    "        total_grad_W2 += grad_W2\n",
    "        \n",
    "    # update the weights\n",
    "    W1 -= learning_rate * total_grad_W1\n",
    "    W2 -= learning_rate * total_grad_W2\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Initial weights and learning rate\n",
    "W1 = 1\n",
    "W2 = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training samples\n",
    "x = [1, 0]\n",
    "y = [1, 0]  \n",
    "\n",
    "# First update\n",
    "W1, W2 = train_step(x, y, W1, W2, learning_rate)\n",
    "\n",
    "print (f\"First update: W1 = {W1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Update the weights of the given neural network using Gradient Descent method\n",
    "\n",
    "- We have one training sampl (1,1). \n",
    "- The initial weights are all 1.\n",
    "- The learning rate is 0.1. \n",
    "- The activation function is the sigmoid function.\n",
    "- The Loss function is the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First update: w1 = 1.0002902919772119\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "# Derivative of the MSE loss function\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Function to perform a forward pass\n",
    "def forward_pass(x, weights):\n",
    "    # Unpacking weights\n",
    "    w1, w2, w3, w4, w5 = weights\n",
    "    \n",
    "    # Forward pass\n",
    "    h1 = sigmoid(w1 * x)\n",
    "    h2 = sigmoid(w2 * h1)\n",
    "    h3 = sigmoid(w3 * h1)\n",
    "    y_pred = sigmoid(w4 * h2 + w5 * h3)\n",
    "    \n",
    "    return h1, h2, h3, y_pred\n",
    "\n",
    "# Function to compute gradients using backward pass\n",
    "def backward_pass(x, y_true, weights, h1, h2, h3, y_pred):\n",
    "    # Unpacking weights\n",
    "    w1, w2, w3, w4, w5 = weights\n",
    "    \n",
    "    # Gradients of the loss with respect to the output\n",
    "    d_loss_y_pred = mse_loss_derivative(y_true, y_pred)\n",
    "    \n",
    "    # Gradients of the output with respect to the pre-activation\n",
    "    d_y_pred_s = sigmoid_derivative(y_pred)\n",
    "    \n",
    "    # Gradients with respect to w4 and w5\n",
    "    d_s_w4 = h2\n",
    "    d_s_w5 = h3\n",
    "    grad_w4 = d_loss_y_pred * d_y_pred_s * d_s_w4\n",
    "    grad_w5 = d_loss_y_pred * d_y_pred_s * d_s_w5\n",
    "    \n",
    "    # Gradients with respect to h2 and h3\n",
    "    d_s_h2 = w4\n",
    "    d_s_h3 = w5\n",
    "    d_loss_h2 = d_loss_y_pred * d_y_pred_s * d_s_h2\n",
    "    d_loss_h3 = d_loss_y_pred * d_y_pred_s * d_s_h3\n",
    "    \n",
    "    # Gradients with respect to the pre-activations h2 and h3\n",
    "    d_h2_s = sigmoid_derivative(h2)\n",
    "    d_h3_s = sigmoid_derivative(h3)\n",
    "    \n",
    "    # Gradients with respect to w2 and w3\n",
    "    d_s_w2 = h1\n",
    "    d_s_w3 = h1\n",
    "    grad_w2 = d_loss_h2 * d_h2_s * d_s_w2\n",
    "    grad_w3 = d_loss_h3 * d_h3_s * d_s_w3\n",
    "    \n",
    "    # Gradients with respect to h1\n",
    "    d_s_h1_2 = w2\n",
    "    d_s_h1_3 = w3\n",
    "    d_loss_h1_2 = d_loss_h2 * d_h2_s * d_s_h1_2\n",
    "    d_loss_h1_3 = d_loss_h3 * d_h3_s * d_s_h1_3\n",
    "    d_loss_h1 = d_loss_h1_2 + d_loss_h1_3\n",
    "    \n",
    "    # Gradients with respect to the pre-activation h1\n",
    "    d_h1_s = sigmoid_derivative(h1)\n",
    "    \n",
    "    # Gradients with respect to w1\n",
    "    d_s_w1 = x\n",
    "    grad_w1 = d_loss_h1 * d_h1_s * d_s_w1\n",
    "    \n",
    "    return grad_w1, grad_w2, grad_w3, grad_w4, grad_w5\n",
    "\n",
    "# Function to perform a single training step\n",
    "def train_step(x, y_true, weights, learning_rate):\n",
    "    # Forward pass\n",
    "    h1, h2, h3, y_pred = forward_pass(x, weights)\n",
    "    \n",
    "    # Backward pass\n",
    "    grads = backward_pass(x, y_true, weights, h1, h2, h3, y_pred)\n",
    "    \n",
    "    # Update weights\n",
    "    updated_weights = [w - learning_rate * grad for w, grad in zip(weights, grads)]\n",
    "    \n",
    "    return updated_weights\n",
    "\n",
    "# Initial weights and learning rate\n",
    "weights = [1, 1, 1, 1, 1]\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training sample\n",
    "x = 1\n",
    "y_true = 1\n",
    "\n",
    "# First update\n",
    "weights = train_step(x, y_true, weights, learning_rate)\n",
    "first_update_w1 = weights[0]\n",
    "\n",
    "print(f\"First update: w1 = {first_update_w1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
