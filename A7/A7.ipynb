{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7 : Support Vector Machines\n",
    "\n",
    "---\n",
    "\n",
    "## 1. the original formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.5000e-01  7.5000e-01  1e+00  1e+00  8e-17\n",
      " 1:  5.8212e-01  9.4382e-01  2e-02  2e-01  2e-16\n",
      " 2:  1.0018e+00  1.0000e+00  2e-03  7e-17  3e-15\n",
      " 3:  1.0000e+00  1.0000e+00  2e-05  1e-17  6e-16\n",
      " 4:  1.0000e+00  1.0000e+00  2e-07  1e-16  6e-16\n",
      "Optimal solution found.\n",
      "Weights: [1. 1.]\n",
      "Bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Data points\n",
    "X = np.array([[0, 1], [-1, 0]])\n",
    "y = np.array([1, -1])\n",
    "\n",
    "# Dimensions\n",
    "m, n = X.shape\n",
    "\n",
    "# Construct the matrices for the QP problem\n",
    "# P matrix for the quadratic part of the objective function (minimizing 0.5 * w.T * w)\n",
    "P = matrix(np.eye(n + 1))  # Include the bias term in the weights matrix\n",
    "q = matrix(\n",
    "    np.zeros(n + 1)\n",
    ")  # Coefficient for the linear part of the objective function (0 bcs only quadratic part)\n",
    "\n",
    "# Append a column for the bias term, b\n",
    "X_extended = np.hstack([X, np.ones((m, 1))])\n",
    "\n",
    "# Construct G so that -y_i * (w . x_i + b) <= -1 (this means y_i * (w . x_i + b) >= 1)\n",
    "G = matrix(-np.multiply(y[:, np.newaxis], X_extended))\n",
    "h = matrix(-np.ones(m))  # The -1 on the right side of the inequalities\n",
    "\n",
    "# Solve QP problem using CVXOPT\n",
    "solution = solvers.qp(P, q, G, h)\n",
    "w_b = np.array(solution[\"x\"]).flatten()  # Retrieve the solution\n",
    "w = w_b[:-1]\n",
    "b = w_b[-1]\n",
    "\n",
    "# Print the results (rounded to 6 decimal places)\n",
    "print(\"Weights:\", np.round(w, 6))  # Weight vector\n",
    "print(\"Bias:\", np.round(b, 6))  # Bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## the dual form of the original formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.5000e-01 -1.7500e+00  1e+00  0e+00  1e+00\n",
      " 1: -9.4382e-01 -9.5908e-01  2e-02  2e-16  2e-01\n",
      " 2: -1.0000e+00 -1.0018e+00  2e-03  0e+00  6e-17\n",
      " 3: -1.0000e+00 -1.0000e+00  2e-05  0e+00  2e-17\n",
      " 4: -1.0000e+00 -1.0000e+00  2e-07  3e-16  9e-17\n",
      "Optimal solution found.\n",
      "Weights: [1. 1.]\n",
      "Bias: -0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Training data\n",
    "X = np.array([[0, 1], [-1, 0]])\n",
    "y = np.array([1, -1])\n",
    "\n",
    "# Dimensions\n",
    "m, n = X.shape\n",
    "\n",
    "# Kernel matrix calculation: y_i * y_j * x_i . x_j\n",
    "# This represents the quadratic term in the dual problem's objective function\n",
    "K = np.dot(X, X.T) * y[:, None] * y[None, :]\n",
    "\n",
    "# Convert K into a cvxopt matrix of type 'd' (double precision floating point)\n",
    "# P matrix: Represents the 0.5 * sum(alpha_i * alpha_j * y_i * y_j * x_i^T * x_j) part of the dual objective\n",
    "P = matrix(K, tc=\"d\")\n",
    "\n",
    "# q vector: Represents the -sum(alpha_i) part of the dual objective\n",
    "q = matrix(-np.ones(m), tc=\"d\")\n",
    "\n",
    "# G matrix: Sets the constraint that alpha_i >= 0 for all i (the inequality constraint)\n",
    "G = matrix(-np.eye(m), tc=\"d\")\n",
    "\n",
    "# h vector: Corresponds to the non-negativity(0) constraint of alpha_i (alpha_i >= 0)\n",
    "h = matrix(np.zeros(m), tc=\"d\")\n",
    "\n",
    "# A matrix: Ensures that the sum of alpha_i * y_i is 0 (the equality constraint)\n",
    "A = matrix(y, (1, m), \"d\")\n",
    "\n",
    "# b scalar: The constant zero in the equality constraint sum(alpha_i * y_i) = 0\n",
    "b = matrix(0.0, tc=\"d\")\n",
    "\n",
    "# Solve QP problem using CVXOPT, which maximizes the dual objective function under the given constraints\n",
    "solution = solvers.qp(P, q, G, h, A, b)\n",
    "alphas = np.array(solution[\"x\"]).flatten()\n",
    "\n",
    "# Compute the weight vector w and the bias b from the solution\n",
    "# Weights calculated as the sum of alpha_i * y_i * x_i, directly using the support vectors and their multipliers\n",
    "w = np.sum(alphas * y[:, None] * X, axis=0)\n",
    "\n",
    "# Find the indices of support vectors (alphas > very small threshold)\n",
    "support_vectors = np.where(alphas > 1e-5)[0]\n",
    "\n",
    "# Calculate the bias using one of the support vectors\n",
    "# Bias is calculated to correct the hyperplane such that it correctly classifies at least one support vector\n",
    "if support_vectors.size > 0:\n",
    "    sv = support_vectors[0]\n",
    "    b = y[sv] - np.dot(\n",
    "        X[sv], w\n",
    "    )  # Use the formula y_sv - (w . x_sv) to determine the correct offset\n",
    "else:\n",
    "    b = 0\n",
    "\n",
    "# Print the results (rounded to 6 decimal places)\n",
    "print(\"Weights:\", np.round(w, 6))  # Weight vector that defines the hyperplane\n",
    "print(\"Bias:\", np.round(b, 6))  # Bias term that offsets the hyperplane"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
