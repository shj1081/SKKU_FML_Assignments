{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 : Logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "## 1. $f(x_1, x_2) = w_1x_1 + w_2x_2 + w_0$\n",
    "\n",
    "the cross entropy error is defined as:\n",
    "\n",
    "$$\n",
    "E(w_1, w_2, w_0) = -\\sum_{x, y \\in D} y\\log(\\sigma(f(x))) + (1 - y)\\log(1 - \\sigma(f(x)))\n",
    "\n",
    "$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "\n",
    "### 1.1 partial derivative of $E(w_1, w_2, w_0)$ with respect to $w_0, w_1, w_2$\n",
    "\n",
    "- $\\frac{\\partial E}{\\partial w_0} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x))) - (1 - y)\\sigma(f(x)) = -\\sum_{x, y \\in D} y - \\sigma(f(x))$\n",
    "- $\\frac{\\partial E}{\\partial w_1} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_1 - (1 - y)\\sigma(f(x))x_1 = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_1$\n",
    "- $\\frac{\\partial E}{\\partial w_2} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_2 - (1 - y)\\sigma(f(x))x_2 = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 code for logistic regression with f(x) = w1x1 + w2x2 + w0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: w0 = 28.800713028060414, w1 = 157.85978452669949, w2 = -71.78437144329715\n",
      "Best fit: f(x) = 28.800713028060414 + 157.85978452669949 * x1 + -71.78437144329715 * x2\n",
      "Cross entropy for (33, 81, 1): 460.51701859880916\n",
      "Cross entropy for (33, 81, 0): -0.0\n",
      "Classification of (33, 81): 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def calc_gradient_logistic(w, given_data):\n",
    "    x = given_data[:, :-1]  # Extracting features (x1, x2)\n",
    "    t = given_data[:, -1]   # Extracting target values\n",
    "    z = np.dot(x, w) # w0 + w1 * x1 + w2 * x2\n",
    "    y_pred = sigmoid(z) # 1 / (1 + np.exp(-z))\n",
    "    error = y_pred - t # y_pred - t\n",
    "    grad = np.dot(x.T, error) # x.T * error (dE/dw0 = x0 * error, dE/dw1 = x1 * error, dE/dw2 = x2 * error)\n",
    "    return grad # [total_dE_dw0, total_dE_dw1, total_dE_dw2]\n",
    "\n",
    "def gradient_descent_logistic(learning_rate, max_iter=1000):\n",
    "    data = np.loadtxt(\"./data.txt\", delimiter=\",\")\n",
    "    x = np.c_[np.ones(data.shape[0]), data[:, :-1]]  # Adding bias term (x0 = 1)\n",
    "    t = data[:, -1]                                   # Extracting target values\n",
    "    w = np.random.rand(x.shape[1]) # Random initialization of w\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        grad = calc_gradient_logistic(w, np.c_[x, t])\n",
    "        # stop when the gradient is small enough\n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            break\n",
    "        w -= learning_rate * grad\n",
    "    return w\n",
    "\n",
    "learning_rate = 0.01\n",
    "solution = gradient_descent_logistic(learning_rate)\n",
    "print(f\"Solution: w0 = {solution[0]}, w1 = {solution[1]}, w2 = {solution[2]}\")\n",
    "print(f\"Best fit: f(x) = {solution[0]} + {solution[1]} * x1 + {solution[2]} * x2\")\n",
    "\n",
    "# Calculate the predicted probability\n",
    "def predict_proba(x, w):\n",
    "    z = np.dot(x, w)\n",
    "    return sigmoid(z)\n",
    "\n",
    "# Calculate cross entropy\n",
    "def calculate_cross_entropy(y_pred, t):\n",
    "    epsilon = 1e-200  # Small epsilon value to prevent taking logarithm of 0\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid logarithm of 0 or 1\n",
    "    return -t * np.log(y_pred) - (1 - t) * np.log(1 - y_pred)\n",
    "\n",
    "\n",
    "# Classify new data points\n",
    "def classify_new_data(x, w):\n",
    "    y_pred = predict_proba(x, w)\n",
    "    return 1 if y_pred >= 0.5 else 0\n",
    "\n",
    "# Calculate cross entropy for (33, 81, 1) and (33, 81, 0)\n",
    "x = np.array([1, 33, 81])\n",
    "t = 1\n",
    "y_pred = predict_proba(x, solution)\n",
    "cross_entropy = calculate_cross_entropy(y_pred, t)\n",
    "print(f\"Cross entropy for (33, 81, 1): {cross_entropy}\")\n",
    "\n",
    "t = 0\n",
    "cross_entropy = calculate_cross_entropy(y_pred, t)\n",
    "print(f\"Cross entropy for (33, 81, 0): {cross_entropy}\")\n",
    "\n",
    "# Classify (33, 81)\n",
    "x = np.array([1, 33, 81])\n",
    "classification = classify_new_data(x, solution)\n",
    "print(f\"Classification of (33, 81): {classification}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. $f(x_1, x_2) = w_5x_{2}^{2} + w_4x_{1}^{2} + w_3x_2x_1 + w_2x_2 + w_1x_1 + w_0$\n",
    "\n",
    "### 2.1 partial derivative of $E(w_0, w_1, w_2, w_3, w_4, w_5)$ with respect to $w_0, w_1, w_2, w_3, w_4, w_5$\n",
    "\n",
    "- $\\frac{\\partial E}{\\partial w_0} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x))) - (1 - y)\\sigma(f(x)) = -\\sum_{x, y \\in D} y - \\sigma(f(x))$\n",
    "- $\\frac{\\partial E}{\\partial w_1} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_1 - (1 - y)\\sigma(f(x))x_1 = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_1$\n",
    "- $\\frac{\\partial E}{\\partial w_2} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_2 - (1 - y)\\sigma(f(x))x_2 = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_2$\n",
    "- $\\frac{\\partial E}{\\partial w_3} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_2x_1 - (1 - y)\\sigma(f(x))x_2x_1 = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_2x_1$\n",
    "- $\\frac{\\partial E}{\\partial w_4} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_{1}^{2} - (1 - y)\\sigma(f(x))x_{1}^{2} = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_{1}^{2}$\n",
    "- $\\frac{\\partial E}{\\partial w_5} = -\\sum_{x, y \\in D} y(1 - \\sigma(f(x)))x_{2}^{2} - (1 - y)\\sigma(f(x))x_{2}^{2} = -\\sum_{x, y \\in D} (y - \\sigma(f(x)))x_{2}^{2}$\n",
    "\n",
    "### 2.2 code for logistic regression with f(x) = $w_5x_{2}^{2} + w_4x_{1}^{2} + w_3x_2x_1 + w_2x_2 + w_1x_1 + w_0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: \n",
      "      w0 = 5.606667383185794, w1 = -425.44074634160273, w2 = -993.8311589976877,w3 = 9985.91012429743, w4 = 10212.596406376351, w5 = -5805.620270135602\n",
      "Best fit: \n",
      "      f(x) = 5.606667383185794 + -425.44074634160273 * x1 + -993.8311589976877 * x2 +9985.91012429743 * x1*x2 + 10212.596406376351 * x1^2 + -5805.620270135602 * x2^2\n",
      "Cross entropy for (33, 81, 1): 460.51701859880916\n",
      "Cross entropy for (33, 81, 0): -0.0\n",
      "Classification of (33, 81): 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "def calc_gradient_logistic(w, given_data):\n",
    "    H = given_data[:, :-1]  # Extracting features (x1, x2, x1*x2, x1^2, x2^2)\n",
    "    t = given_data[:, -1]   # Extracting target values\n",
    "    z = np.dot(H, w) # Corrected variable name\n",
    "    y_pred = sigmoid(z)\n",
    "    error = y_pred - t\n",
    "    grad = np.dot(H.T, error)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent_logistic(learning_rate, max_iter=1000):\n",
    "    data = np.loadtxt(\"./data.txt\", delimiter=\",\")\n",
    "    H = np.c_[\n",
    "            np.ones(data.shape[0]), # Adding bias term (x0 = 1)\n",
    "            data[:, 0], \n",
    "            data[:, 1], \n",
    "            data[:, 0] * data[:, 1], \n",
    "            data[:, 0] ** 2, \n",
    "            data[:, 1] ** 2\n",
    "            ]\n",
    "    t = data[:, -1]\n",
    "    w = np.random.rand(H.shape[1])\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        grad = calc_gradient_logistic(w, np.c_[H, t])\n",
    "        # stop when the gradient is small enough\n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            break\n",
    "        w -= learning_rate * grad\n",
    "    return w\n",
    "\n",
    "learning_rate = 0.01\n",
    "solution = gradient_descent_logistic(learning_rate)\n",
    "print(f\"\"\"Solution: \n",
    "      w0 = {solution[0]}, w1 = {solution[1]}, w2 = {solution[2]},w3 = {solution[3]}, w4 = {solution[4]}, w5 = {solution[5]}\"\"\")\n",
    "print(f\"\"\"Best fit: \n",
    "      f(x) = {solution[0]} + {solution[1]} * x1 + {solution[2]} * x2 +{solution[3]} * x1*x2 + {solution[4]} * x1^2 + {solution[5]} * x2^2\"\"\")\n",
    "\n",
    "\n",
    "# Calculate cross entropy for (33, 81, 1) and (33, 81, 0)\n",
    "x = np.array([1, 33, 81, 33 * 81, 33 ** 2, 81 ** 2])\n",
    "t = 1\n",
    "y_pred = predict_proba(x, solution)\n",
    "cross_entropy = calculate_cross_entropy(y_pred, t)\n",
    "print(f\"Cross entropy for (33, 81, 1): {cross_entropy}\")\n",
    "\n",
    "t = 0\n",
    "cross_entropy = calculate_cross_entropy(y_pred, t)\n",
    "print(f\"Cross entropy for (33, 81, 0): {cross_entropy}\")\n",
    "\n",
    "# Classify (33, 81)\n",
    "x = np.array([1, 33, 81, 33 * 81, 33 ** 2, 81 ** 2])\n",
    "classification = classify_new_data(x, solution)\n",
    "print(f\"Classification of (33, 81): {classification}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
